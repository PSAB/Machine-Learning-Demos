{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-01 09:59:28.018690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import our standard libraries.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style='darkgrid')  # default style\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$. Each datapoint represents the value of a single feature. Because we'll want our model to have a learned *bias* (or *intercept*), we will use an extra feature which will always be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are our inputs.\n",
    "X = np.array([[1, 3],\n",
    "              [1, 2]])\n",
    "Y = np.array([7, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a linear regression model: f(x) = w0 + w1*x1\n",
    "W = np.array([1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions\n",
    "Suppose we are using linear regression. Then the functional form for our model is: \n",
    "\n",
    "\\begin{align}\n",
    "h(x) &= w_0 + w_1x \n",
    "\\end{align}\n",
    "\n",
    "Since we're using an extra feature corresponding to the intercept/bias, we can write:\n",
    "\n",
    "\\begin{align}\n",
    "h(x) &= w_0x_0 + w_1x_1 \\\\\n",
    "\\end{align}\n",
    "\n",
    "Given parameter values, practice computing model predictions.\n",
    "\n",
    "In linear regression, when including a bias term (intercept), we often set one of the input features to a constant value, typically 1. This is done to account for the constant term in the linear equation and to make the mathematics more convenient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our model function with matrix multiplication and using the notation $h_W$ to make clear that our model is *parameterized* by $W$ (the vector of parameters):\n",
    "\n",
    "\\begin{align}\n",
    "h_W(x) = w_0x_0 + w_1x_1 = xW^T\n",
    "\\end{align}\n",
    "\n",
    "To make this matrix formulation as clear as possible, this is:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = h_W(x) = xW^T =\n",
    "\\begin{pmatrix}\n",
    "x_0 & x_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h_W(X) = XW^T =\n",
    "\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} \\\\\n",
    "x_{1,0} & x_{1,1} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n",
    "\n",
    "\\begin{align}\n",
    "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
    "\\end{align}\n",
    "\n",
    "Let's use numpy functions to compute predictions for both X examples at once. The result should be a vector with 2 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3]\n"
     ]
    }
   ],
   "source": [
    "# Compute predictions for all X examples at once.\n",
    "#X: 2x2\n",
    "#W.T: 2x1 \n",
    "#Y_hat: 2x1\n",
    "preds = np.dot(X, W.T) # The matrix multiplication of X and the transpose of the weights/parameters matrix gives the model predictions.\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "Use numpy functions to compute a vector of differences between model predictions and labels (values of Y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3 -2]\n"
     ]
    }
   ],
   "source": [
    "diff = preds - Y # This order matters since it gives the correct direction for the gradient \n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the MSE loss. The result should be a single (scalar) value. Remember we're using this formula (see assignment 1):\n",
    "\n",
    "\\begin{align}\n",
    "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "where we've changed the standard scaling from $\\frac{1}{m}$ to $\\frac{1}{2m}$, where $m$ is the number of examples (2, in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.5\n"
     ]
    }
   ],
   "source": [
    "# Get the number of examples\n",
    "m = X.shape[0]\n",
    "\n",
    "# Compute the average per-example loss\n",
    "loss = np.sum(diff**2) / m\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient\n",
    "\n",
    "Refer to assignment 1 or the gradient descent lecture for the derivation, but here's the formula for the partial derivatives (which together form the gradient):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_j} J(W) &= (h_W(x) - y)x_j\n",
    "\\end{align}\n",
    "\n",
    "This formula is assuming we only have a single example. The general formula has an average over examples:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{1}{m}\\sum_i(h_W(x^{(i)}) - y^{(i)})x^{(i)}_j\n",
    "\\end{align}\n",
    "\n",
    "You're ready to compute the gradient. The result will be a vector of partial derivatives for $w_0$ and $w_1$ respectively. You can use matrix computations as before.\n",
    "\n",
    "\\begin{align}\n",
    "\\nabla J(W) &= \\frac{1}{m}(h_W(X) - Y)X\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.5 -6.5]\n"
     ]
    }
   ],
   "source": [
    "# compute the gradient\n",
    "gradient = np.dot(diff, X) / m # The dot product, by defintion, is a summation\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter updates\n",
    "Now that you've computed the gradient, apply parameter updates by subtracting the appropriate partial derivatives (scaled by a learning rate) from the initial parameter values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.25 1.65]\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "W = W - (learning_rate * gradient)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression (multiple features)\n",
    "\n",
    "Suppose we have a dataset with 2 datapoints, $x^{(0)}$ and $x^{(1)}$, but now we have multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are our inputs:\n",
    "X = np.array([[1, 3, 1, 1],\n",
    "              [1, 2, 2, 0]])\n",
    "Y = np.array([2, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write out our model function:\n",
    "\n",
    "\\begin{align}\n",
    "h_W(x) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 = xW^T\n",
    "\\end{align}\n",
    "\n",
    "We can get all predictions with this matrix product:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = h_W(X) = XW^T =\n",
    "\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} & x_{0,2} & x_{0,3} \\\\\n",
    "x_{1,0} & x_{1,1} & x_{1,2} & x_{1,3} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} & x_{m-1,2} & x_{m-1,3} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "w_3 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Given the (initial) parameter values below, compute the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 5]\n"
     ]
    }
   ],
   "source": [
    "# Initial parameter values\n",
    "W = np.array([1, 1, 1, 1])\n",
    "\n",
    "# Compute predictions\n",
    "preds = np.dot(X, W.T)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4. 10.  6.  2.]\n"
     ]
    }
   ],
   "source": [
    "m, n = X.shape\n",
    "\n",
    "diff = preds - Y\n",
    "gradient = np.dot(diff, X) / m\n",
    "print(gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now put everything together in gradient descent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preds: [6 5]\n",
      "loss: 16.0\n",
      "gradient: [ 4. 10.  6.  2.]\n",
      "weights: [0.6 0.  0.4 0.8]\n",
      "\n",
      "preds: [1.8 1.4]\n",
      "loss: 0.09999999999999999\n",
      "gradient: [ 0.1  0.1  0.3 -0.1]\n",
      "weights: [ 0.59 -0.01  0.37  0.81]\n",
      "\n",
      "preds: [1.74 1.31]\n",
      "loss: 0.08185000000000002\n",
      "gradient: [ 0.025 -0.08   0.18  -0.13 ]\n",
      "weights: [ 0.5875 -0.002   0.352   0.823 ]\n",
      "\n",
      "preds: [1.7565 1.2875]\n",
      "loss: 0.07097424999999997\n",
      "gradient: [ 0.022   -0.07775  0.16575 -0.12175]\n",
      "weights: [0.5853   0.005775 0.335425 0.835175]\n",
      "\n",
      "preds: [1.773225 1.2677  ]\n",
      "loss: 0.061545095312499944\n",
      "gradient: [ 0.0204625 -0.0724625  0.1543125 -0.1133875]\n",
      "weights: [0.58325375 0.01302125 0.31999375 0.84651375]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "\n",
    "for i in range(5): # Each iteration represents an epoch\n",
    "    preds = np.dot(X, W.T)\n",
    "    loss = np.mean((preds - Y)**2)\n",
    "    gradient = np.dot((preds - Y), X) / m\n",
    "    W = W - learning_rate * gradient\n",
    "\n",
    "    print(\"preds:\", preds)\n",
    "    print(\"loss:\", loss)\n",
    "    print(\"gradient:\", gradient)\n",
    "    print(\"weights:\", W)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with TensorFlow/Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Initilize a model with a single dense layer\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(\n",
    "    units = 1,              # output dim\n",
    "    input_shape = [4],      # input dim\n",
    "    use_bias=False,         # We already included the bias in X\n",
    "    kernel_initializer=tf.ones_initializer # Initialize weights to 1\n",
    "))\n",
    "\n",
    "# Compile the model with an optimizer that specifies learning_rate = 0.1, and also loss = 'mse'\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 75ms/step\n",
      "predictions: [[-2.4 -2.2]]\n",
      "loss: 16.0\n",
      "W: [[ 0.19999999 -1.         -0.20000005  0.6       ]]\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  x = X,\n",
    "  y = Y,\n",
    "  epochs=1,\n",
    "  batch_size=2,\n",
    "  verbose=0)\n",
    "loss = history.history['loss'][0]\n",
    "weights = model.layers[0].get_weights()[0].T\n",
    "preds = model.predict(X)\n",
    "\n",
    "print('predictions:', preds.T)\n",
    "print('loss:', loss)\n",
    "print('W:', weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_demos_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
